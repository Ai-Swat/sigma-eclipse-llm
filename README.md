<div align="center">

<img src="src/assets/logo2.png" alt="Sigma Eclipse Logo" width="128" height="128">

# Sigma Eclipse LLM

**Run powerful AI locally â€” no cloud, no limits, complete privacy.**

[![License](https://img.shields.io/badge/license-PolyForm%20NC-blue.svg)](LICENSE)
[![Platform](https://img.shields.io/badge/platform-macOS%20%7C%20Windows-blue)](#installation)
[![Tauri](https://img.shields.io/badge/built%20with-Tauri%202-FFC131?logo=tauri)](https://tauri.app/)
[![llama.cpp](https://img.shields.io/badge/powered%20by-llama.cpp-green)](https://github.com/ggerganov/llama.cpp)

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [How It Works](#-how-it-works) â€¢ [Development](#-development) â€¢ [License](#-license)

---

</div>

## ğŸŒ Part of Sigma Eclipse Ecosystem

This application is a core component of the **Sigma Eclipse** project â€” a privacy-focused AI ecosystem. It works seamlessly with:

- **[Sigma Browser](https://www.sigmabrowser.com/)** â€” A privacy-first browser with built-in AI capabilities
- **[Sigma Eclipse Extension](https://github.com/Ai-Swat/sigma-eclipse-extension)** â€” Browser extension that connects to this local LLM server

Together, these components provide a complete solution for running AI locally while browsing the web, ensuring your data stays on your machine.

## ğŸš€ What is Sigma Eclipse LLM?

Sigma Eclipse LLM is a lightweight desktop application that lets you run large language models (LLMs) locally on your machine. No API keys, no subscriptions, no data leaving your computer â€” just pure, private AI at your fingertips.

Built with [Tauri](https://tauri.app/) and powered by [llama.cpp](https://github.com/ggerganov/llama.cpp), Sigma Eclipse LLM combines native performance with a beautiful, intuitive interface.


## âœ¨ Features

### ğŸ¯ Dead Simple
- **One-click setup** â€” automatically downloads everything you need
- **Zero configuration** â€” smart defaults that just work
- **Clean interface** â€” no clutter, no confusion

### ğŸ”’ Privacy First
- **100% local** â€” your data never leaves your machine
- **No accounts** â€” no sign-ups, no tracking, no telemetry
- **Offline capable** â€” works without internet after initial setup

### âš¡ Powerful
- **GPU acceleration** â€” automatic GPU detection and optimization
- **Multiple models** â€” switch between models easily
- **Native performance** â€” Rust backend with minimal resource usage
- **Browser integration** â€” seamless connection with Sigma browser extension

### ğŸŒ Cross-Platform
- **macOS** (Apple Silicon)
- **Windows** (x64)

## ğŸ“¦ Installation

### Download

Download the latest release for your platform:

| Platform | Download |
|----------|----------|
| macOS (ARM) | [Sigma Eclipse.dmg](https://github.com/ai-swat/sigma-eclipse-llm/releases/latest) |
| Windows | [Sigma Eclipse Setup.exe](https://github.com/ai-swat/sigma-eclipse-llm/releases/latest) |

### First Launch

1. **Open Sigma Eclipse**
2. **Wait for automatic setup** â€” the app downloads llama.cpp and the default model (~3-6 GB)
3. **Click "Start"** â€” your local AI server is now running!

That's it. No terminal commands, no manual downloads, no config files.

## ğŸ”§ How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Sigma Eclipse                            â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   React UI   â”‚â—„â”€â”€â–ºâ”‚  Tauri Core  â”‚â—„â”€â”€â–ºâ”‚  llama.cpp   â”‚   â”‚
â”‚  â”‚  (Frontend)  â”‚    â”‚    (Rust)    â”‚    â”‚   (Server)   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                               â”‚
â”‚                              â–¼                               â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚                   â”‚  Native Messaging â”‚                      â”‚
â”‚                   â”‚   (Browser API)   â”‚                      â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Sigma Eclipse** manages a local llama.cpp server that provides an OpenAI-compatible API. This means:

- ğŸŒ **Local API endpoint** at `http://localhost:8080`
- ğŸ”Œ **Compatible** with any tool that supports OpenAI API
- ğŸ§© **Native messaging** enables browser extensions to communicate directly

## âš™ï¸ Configuration

Access settings via the âš™ï¸ gear icon:

| Setting | Description | Default |
|---------|-------------|---------|
| **Context Size** | Maximum conversation context (tokens) | Auto-detected |
| **GPU Layers** | Number of layers offloaded to GPU | Auto-detected |
| **Model** | Select from available models | Gemma 2B |

> ğŸ’¡ **Tip:** Sigma Eclipse automatically detects your hardware and suggests optimal settings.

## ğŸ› ï¸ Development

### Prerequisites

- [Node.js](https://nodejs.org/) v18+
- [Rust](https://rustup.rs/) (latest stable)
- Platform-specific dependencies:
  - **macOS:** `xcode-select --install`
  - **Windows:** Visual Studio C++ Build Tools

### Quick Start

```bash
# Clone the repository
git clone https://github.com/ai-swat/sigma-eclipse-llm.git
cd sigma-eclipse

# Install dependencies
npm install

# Run in development mode
npm run tauri dev
```

### Build for Production

```bash
npm run tauri build
```

Built artifacts will be in `src-tauri/target/release/bundle/`

### Project Structure

```
sigma-eclipse/
â”œâ”€â”€ src/                    # React frontend
â”‚   â”œâ”€â”€ components/         # UI components
â”‚   â”œâ”€â”€ hooks/              # React hooks
â”‚   â”œâ”€â”€ styles/             # CSS styles
â”‚   â””â”€â”€ types/              # TypeScript types
â”œâ”€â”€ src-tauri/              # Rust backend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.rs         # Entry point
â”‚   â”‚   â”œâ”€â”€ server.rs       # LLM server management
â”‚   â”‚   â”œâ”€â”€ download/       # Model & binary downloads
â”‚   â”‚   â””â”€â”€ native_messaging.rs
â”‚   â””â”€â”€ tauri.conf.json     # Tauri configuration
â””â”€â”€ package.json
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“œ License

This project is licensed under the [PolyForm Noncommercial License 1.0.0](LICENSE).

**TL;DR:** Free for personal, educational, and non-commercial use. Contact us for commercial licensing.

## ğŸ™ Acknowledgments

- [llama.cpp](https://github.com/ggerganov/llama.cpp) â€” The amazing LLM inference engine
- [Tauri](https://tauri.app/) â€” Framework for building tiny, fast desktop apps
- [Hugging Face](https://huggingface.co/) â€” Model hosting and community

---

<div align="center">

**Made with â¤ï¸ by [AI SWAT](https://github.com/ai-swat)**

[â¬† Back to Top](#sigma-eclipse-llm)

</div>
